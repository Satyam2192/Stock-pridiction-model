--- File: /home/sk/Desktop/Stock pridiction model/extracted_text.txt ---



--- File: /home/sk/Desktop/Stock pridiction model/requirements.txt ---

pandas 
numpy 
matplotlib 
seaborn 
scikit-learn 
tensorflow 
keras 
statsmodels

--- File: /home/sk/Desktop/Stock pridiction model/scripts/evaluate.py ---

import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import joblib
import os

def evaluate_model():
    # Load the preprocessed data
    X_test = pd.read_csv('../data/preprocessed_stock_data_X_test.csv')
    y_test = pd.read_csv('../data/preprocessed_stock_data_y_test.csv')
    dates_test = pd.read_csv('../data/preprocessed_stock_data_dates_test.csv')
    
    # Convert dates back to datetime
    dates_test['日付け'] = pd.to_datetime(dates_test['日付け'])

    # Load the saved model
    model = load_model('../models/final_stock_price_model.keras')

    # Reshape X_test
    X_test_reshaped = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))

    # Make predictions
    predictions = model.predict(X_test_reshaped)

    # Load the scaler and inverse transform the predictions and actual values
    scaler = joblib.load('../data/preprocessed_stock_data_scaler.joblib')
    
    # Create dummy arrays for inverse transform
    dummy = np.zeros((len(predictions), scaler.scale_.shape[0]))
    dummy[:, 0] = predictions.flatten()  # Assuming '終値' is the first column
    predictions_original = scaler.inverse_transform(dummy)[:, 0]
    
    dummy[:, 0] = y_test.values.flatten()
    y_test_original = scaler.inverse_transform(dummy)[:, 0]

    # Calculate metrics
    mse = mean_squared_error(y_test_original, predictions_original)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test_original, predictions_original)
    r2 = r2_score(y_test_original, predictions_original)

    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
    print(f"Mean Absolute Error (MAE): {mae:.2f}")
    print(f"R-squared (R²): {r2:.4f}")
    print(f"Model Accuracy: {r2*100:.2f}%")

    # Ensure results directory exists
    os.makedirs('../results', exist_ok=True)

    # Save evaluation results
    evaluation_results = pd.DataFrame({
        'Date': dates_test['日付け'],
        'Actual_Prices': y_test_original,
        'Predicted_Prices': predictions_original,
        'Error': y_test_original - predictions_original
    })
    evaluation_results.to_csv('../results/evaluation_results.csv', index=False)

    # Plot predictions vs actuals
    plt.figure(figsize=(15, 7))
    plt.plot(dates_test['日付け'], y_test_original, label='Actual Prices', color='blue')
    plt.plot(dates_test['日付け'], predictions_original, label='Predicted Prices', color='orange')
    plt.title('Stock Price Predictions vs Actual Prices')
    plt.xlabel('Date')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('../results/predictions_vs_actuals.png')
    plt.close()

if __name__ == "__main__":
    try:
        evaluate_model()
        print("Evaluation completed successfully!")
    except Exception as e:
        print(f"An error occurred during evaluation: {str(e)}")



--- File: /home/sk/Desktop/Stock pridiction model/scripts/model_improvement.py ---

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import os

# Suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

def load_data(X_file, y_file):
    X = pd.read_csv(X_file)
    y = pd.read_csv(y_file).values.flatten()
    X_values = X.drop(columns=['日付け']).values
    X_values = np.reshape(X_values, (X_values.shape[0], 1, X_values.shape[1]))
    return X_values, y

def build_improved_model(input_shape, lstm_units=50, dense_units=25, dropout_rate=0.2, learning_rate=0.001):
    model = Sequential([
        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        LSTM(lstm_units, return_sequences=False),
        Dropout(dropout_rate),
        Dense(dense_units),
        Dense(1)
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')
    return model

def train_and_evaluate_model(X, y, model, epochs=100, batch_size=32):
    tscv = TimeSeriesSplit(n_splits=5)
    mse_scores = []

    for train_index, val_index in tscv.split(X):
        X_train, X_val = X[train_index], X[val_index]
        y_train, y_val = y[train_index], y[val_index]

        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping],
            verbose=1
        )
        
        val_predictions = model.predict(X_val)
        val_mse = mean_squared_error(y_val, val_predictions)
        mse_scores.append(val_mse)

    return model, np.mean(mse_scores)

if __name__ == "__main__":
    X, y = load_data('../data/preprocessed_stock_data_X_train.csv', '../data/preprocessed_stock_data_y_train.csv')
    model = build_improved_model((X.shape[1], X.shape[2]))
    model, avg_mse = train_and_evaluate_model(X, y, model)
    model.save('../models/improved_stock_price_model.keras')
    print(f"Average MSE from cross-validation: {avg_mse}")


--- File: /home/sk/Desktop/Stock pridiction model/scripts/model_train.py ---

import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os

def load_data(X_file, y_file):
    X = pd.read_csv(X_file).values
    y = pd.read_csv(y_file).values.flatten()
    X = np.reshape(X, (X.shape[0], 1, X.shape[1]))
    return X, y

def build_model(input_shape):
    model = Sequential([
        LSTM(100, return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        LSTM(100, return_sequences=False),
        Dropout(0.2),
        Dense(50),
        Dense(1)
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

def train_model():
    # Load data
    X_train, y_train = load_data(
        '../data/preprocessed_stock_data_X_train.csv',
        '../data/preprocessed_stock_data_y_train.csv'
    )
    X_test, y_test = load_data(
        '../data/preprocessed_stock_data_X_test.csv',
        '../data/preprocessed_stock_data_y_test.csv'
    )

    # Create model
    model = build_model((X_train.shape[1], X_train.shape[2]))
    
    # Prepare callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    model_checkpoint = ModelCheckpoint(
        '../models/best_model.keras',
        monitor='val_loss',
        save_best_only=True
    )

    # Train model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=100,
        batch_size=32,
        callbacks=[early_stopping, model_checkpoint],
        verbose=1
    )
    
    # Save the final model
    model.save('../models/final_stock_price_model.keras')
    
    return history

if __name__ == "__main__":
    try:
        os.makedirs('../models', exist_ok=True)
        history = train_model()
        print("Model training completed successfully!")
    except Exception as e:
        print(f"An error occurred during training: {str(e)}")

--- File: /home/sk/Desktop/Stock pridiction model/scripts/preprocess.py ---

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib
import os

def convert_to_float(value):
    """Convert string values like '220.24M' or '-2.56%' to float."""
    if isinstance(value, str):
        value = value.strip()
        if value.endswith('M'):
            return float(value[:-1]) * 1_000_000
        elif value.endswith('B'):
            return float(value[:-1]) * 1_000_000_000
        elif value.endswith('%'):
            return float(value[:-1]) / 100
        else:
            try:
                return float(value)
            except ValueError:
                return np.nan
    return value

def preprocess_data(input_file, output_file_prefix):
    # Load the dataset
    df = pd.read_csv(input_file)
    
    print("Original columns:", df.columns.tolist())
    
    # Convert date column
    df['日付け'] = pd.to_datetime(df['日付け'])
    df = df.sort_values('日付け')
    
    # Clean and convert numerical columns
    numerical_columns = ['終値', '始値', '高値', '安値', '出来高']
    for col in numerical_columns:
        df[col] = df[col].apply(convert_to_float)
    
    # Convert change rate column (handles the space in the column name)
    change_rate_col = '変化率 %'
    if change_rate_col in df.columns:
        df[change_rate_col] = df[change_rate_col].apply(convert_to_float)
    
    # Create features
    df['MA_5'] = df['終値'].rolling(window=5).mean()
    df['MA_20'] = df['終値'].rolling(window=20).mean()
    df['Volatility'] = df['終値'].rolling(window=5).std()
    
    # Create time-based features
    df['DayOfWeek'] = df['日付け'].dt.dayofweek
    df['Month'] = df['日付け'].dt.month
    
    # Calculate price range
    df['PriceRange'] = df['高値'] - df['安値']
    
    # Drop NaN values
    df.dropna(inplace=True)

    # Prepare features for scaling
    feature_columns = ['終値', '始値', '高値', '安値', '出来高', 'MA_5', 'MA_20', 
                       'Volatility', 'PriceRange']
    
    # Add change rate to features
    if change_rate_col in df.columns:
        feature_columns.append(change_rate_col)
    
    # Scaling
    scaler = MinMaxScaler()
    df[feature_columns] = scaler.fit_transform(df[feature_columns])
    
    # Ensure output directory exists
    os.makedirs(os.path.dirname(output_file_prefix), exist_ok=True)
    
    # Save scaler
    joblib.dump(scaler, f'{output_file_prefix}_scaler.joblib')

    # Prepare final feature set
    X = df[feature_columns + ['DayOfWeek', 'Month']]
    y = df['終値']
    dates = df['日付け']

    # Split the data while preserving time order
    train_size = int(len(df) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    dates_train, dates_test = dates[:train_size], dates[train_size:]

    # Save preprocessed data
    X_train.to_csv(f'{output_file_prefix}_X_train.csv', index=False)
    X_test.to_csv(f'{output_file_prefix}_X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv(f'{output_file_prefix}_y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv(f'{output_file_prefix}_y_test.csv', index=False)
    pd.DataFrame(dates_train).to_csv(f'{output_file_prefix}_dates_train.csv', index=False)
    pd.DataFrame(dates_test).to_csv(f'{output_file_prefix}_dates_test.csv', index=False)
    
    print(f"\nData preprocessing completed successfully!")
    print(f"Training set size: {len(X_train)}")
    print(f"Test set size: {len(X_test)}")
    print(f"Features used: {feature_columns + ['DayOfWeek', 'Month']}")

if __name__ == "__main__":
    input_file = '../data/stock_price.csv'
    output_prefix = '../data/preprocessed_stock_data'
    
    try:
        preprocess_data(input_file, output_prefix)
    except Exception as e:
        print(f"An error occurred: {str(e)}")

